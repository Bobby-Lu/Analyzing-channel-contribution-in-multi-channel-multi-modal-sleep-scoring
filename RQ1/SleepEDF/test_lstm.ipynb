{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-20-4f73720b1348>:104: DeprecationWarning: Call to deprecated function get_sheet_by_name (Use wb[sheetname]).\n",
      "  sheet1 = wb.get_sheet_by_name('Sheet 1')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test dataset loaded\n",
      "Total Length instances in generator: 2809\n",
      "New Patient\n",
      "190\n",
      "New Patient\n",
      "121\n",
      "conf_mat_test: [[ 519.   21.   17.    2.    5.]\n",
      " [  49.   98.   27.    0.   16.]\n",
      " [   3.   26. 1185.   23.   30.]\n",
      " [   0.    0.   15.  155.    0.]\n",
      " [  25.   41.   13.    0.  539.]]\n",
      "total_30sec_epochs_test: 2809\n",
      "\n",
      "Test set: Average loss: 0.3595, Accuracy: 2496/2809 (89%), Epoch:201\n",
      "\n",
      "test dataset loaded\n",
      "Total Length instances in generator: 2809\n",
      "New Patient\n",
      "190\n",
      "New Patient\n",
      "121\n",
      "conf_mat_test: [[ 481.   57.   17.    2.    7.]\n",
      " [  38.  102.   26.    0.   24.]\n",
      " [   2.   25. 1172.   36.   32.]\n",
      " [   0.    0.   11.  159.    0.]\n",
      " [   7.   28.   13.    0.  570.]]\n",
      "total_30sec_epochs_test: 2809\n",
      "\n",
      "Test set: Average loss: 0.2987, Accuracy: 2484/2809 (88%), Epoch:201\n",
      "\n",
      "test dataset loaded\n",
      "Total Length instances in generator: 2809\n",
      "New Patient\n",
      "190\n",
      "New Patient\n",
      "121\n",
      "conf_mat_test: [[4.910e+02 4.400e+01 1.900e+01 4.000e+00 6.000e+00]\n",
      " [3.000e+01 1.130e+02 2.500e+01 0.000e+00 2.200e+01]\n",
      " [1.000e+00 3.000e+01 1.173e+03 2.800e+01 3.500e+01]\n",
      " [0.000e+00 0.000e+00 1.300e+01 1.570e+02 0.000e+00]\n",
      " [7.000e+00 3.100e+01 1.200e+01 0.000e+00 5.680e+02]]\n",
      "total_30sec_epochs_test: 2809\n",
      "\n",
      "Test set: Average loss: 0.3261, Accuracy: 2502/2809 (89%), Epoch:201\n",
      "\n",
      "test dataset loaded\n",
      "Total Length instances in generator: 2809\n",
      "New Patient\n",
      "190\n",
      "New Patient\n",
      "121\n",
      "conf_mat_test: [[ 489.   44.   23.    2.    6.]\n",
      " [  40.   99.   30.    0.   21.]\n",
      " [   2.   20. 1173.   43.   29.]\n",
      " [   0.    0.   12.  158.    0.]\n",
      " [  14.   47.   16.    0.  541.]]\n",
      "total_30sec_epochs_test: 2809\n",
      "\n",
      "Test set: Average loss: 0.3690, Accuracy: 2460/2809 (88%), Epoch:201\n",
      "\n",
      "test dataset loaded\n",
      "Total Length instances in generator: 2809\n",
      "New Patient\n",
      "190\n",
      "New Patient\n",
      "121\n",
      "conf_mat_test: [[ 504.   33.   15.    3.    9.]\n",
      " [  46.   93.   24.    0.   27.]\n",
      " [   4.   19. 1178.   34.   32.]\n",
      " [   0.    0.   10.  160.    0.]\n",
      " [   8.   59.   15.    0.  536.]]\n",
      "total_30sec_epochs_test: 2809\n",
      "\n",
      "Test set: Average loss: 0.3254, Accuracy: 2471/2809 (88%), Epoch:201\n",
      "\n",
      "test dataset loaded\n",
      "Total Length instances in generator: 2809\n",
      "New Patient\n",
      "190\n",
      "New Patient\n",
      "121\n",
      "conf_mat_test: [[5.090e+02 2.900e+01 1.800e+01 2.000e+00 6.000e+00]\n",
      " [4.100e+01 1.060e+02 2.300e+01 0.000e+00 2.000e+01]\n",
      " [1.000e+00 2.000e+01 1.173e+03 3.600e+01 3.700e+01]\n",
      " [0.000e+00 0.000e+00 1.100e+01 1.590e+02 0.000e+00]\n",
      " [1.100e+01 5.800e+01 1.300e+01 0.000e+00 5.360e+02]]\n",
      "total_30sec_epochs_test: 2809\n",
      "\n",
      "Test set: Average loss: 0.3270, Accuracy: 2483/2809 (88%), Epoch:201\n",
      "\n",
      "test dataset loaded\n",
      "Total Length instances in generator: 2809\n",
      "New Patient\n",
      "190\n",
      "New Patient\n",
      "121\n",
      "conf_mat_test: [[4.990e+02 3.200e+01 2.300e+01 3.000e+00 7.000e+00]\n",
      " [4.700e+01 8.900e+01 3.000e+01 0.000e+00 2.400e+01]\n",
      " [1.000e+00 1.800e+01 1.193e+03 2.800e+01 2.700e+01]\n",
      " [0.000e+00 0.000e+00 1.500e+01 1.550e+02 0.000e+00]\n",
      " [9.000e+00 4.600e+01 1.400e+01 0.000e+00 5.490e+02]]\n",
      "total_30sec_epochs_test: 2809\n",
      "\n",
      "Test set: Average loss: 0.3300, Accuracy: 2485/2809 (88%), Epoch:201\n",
      "\n",
      "test dataset loaded\n",
      "Total Length instances in generator: 2809\n",
      "New Patient\n",
      "190\n",
      "New Patient\n",
      "121\n",
      "conf_mat_test: [[ 499.   41.   17.    2.    5.]\n",
      " [  32.   94.   35.    0.   29.]\n",
      " [   3.   15. 1173.   34.   42.]\n",
      " [   0.    0.   12.  158.    0.]\n",
      " [   5.   55.   15.    0.  543.]]\n",
      "total_30sec_epochs_test: 2809\n",
      "\n",
      "Test set: Average loss: 0.3564, Accuracy: 2467/2809 (88%), Epoch:201\n",
      "\n",
      "test dataset loaded\n",
      "Total Length instances in generator: 2809\n",
      "New Patient\n",
      "190\n",
      "New Patient\n",
      "121\n",
      "conf_mat_test: [[ 491.   29.   34.    2.    8.]\n",
      " [  39.   84.   47.    0.   20.]\n",
      " [   3.    7. 1204.   25.   28.]\n",
      " [   0.    0.   15.  155.    0.]\n",
      " [   7.   31.   46.    0.  534.]]\n",
      "total_30sec_epochs_test: 2809\n",
      "\n",
      "Test set: Average loss: 0.3520, Accuracy: 2468/2809 (88%), Epoch:201\n",
      "\n",
      "test dataset loaded\n",
      "Total Length instances in generator: 2809\n",
      "New Patient\n",
      "190\n",
      "New Patient\n",
      "121\n",
      "conf_mat_test: [[ 498.   36.   23.    2.    5.]\n",
      " [  30.  100.   40.    0.   20.]\n",
      " [   2.   11. 1206.   21.   27.]\n",
      " [   0.    0.   20.  150.    0.]\n",
      " [  11.   58.   23.    0.  526.]]\n",
      "total_30sec_epochs_test: 2809\n",
      "\n",
      "Test set: Average loss: 0.3212, Accuracy: 2480/2809 (88%), Epoch:201\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import h5py\n",
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "import utils\n",
    "import cnn_models\n",
    "import lstm_models\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader,SequentialSampler\n",
    "import os\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import pickle\n",
    "import math\n",
    "from collections import OrderedDict\n",
    "from openpyxl import Workbook\n",
    "import openpyxl as op\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def load_pretrained_model_for_LSTM(model,path):\n",
    "    state_dict_new_model=model.state_dict()\n",
    "    checkpoint = torch.load(path,map_location=torch.device('cpu'))\n",
    "    state_dict_pretrained=checkpoint['state_dict']\n",
    "    state_dict_remove_module = OrderedDict()\n",
    "    for k, v in state_dict_pretrained.items():\n",
    "        if k!='linear.weight' and k!='linear.bias':\n",
    "            #k = k[7:] # remove `module.`\n",
    "            state_dict_remove_module[k] = v\n",
    "    state_dict_new_model.update(state_dict_remove_module)\n",
    "    model.load_state_dict(state_dict_new_model)\n",
    "    return model\n",
    "\n",
    "def loss_fn():\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    return criterion\n",
    "\n",
    "def conf_mat_create(predicted,true,correct,total_30sec_epochs,conf_mat):\n",
    "    total_30sec_epochs+=true.size()[0]\n",
    "    correct += predicted.eq(true.view_as(predicted)).sum().item()\n",
    "    conf_mat=conf_mat+confusion_matrix(true.cpu().numpy(),predicted.cpu().numpy(),labels=classes)\n",
    "    return correct, total_30sec_epochs,conf_mat\n",
    "\n",
    "\n",
    "def test(cnn_model, lstm_model, data_test, epoch):\n",
    "    cnn_model.eval()\n",
    "    lstm_model.eval()\n",
    "    total_30sec_epochs_test = 0\n",
    "    test_loss = 0.0\n",
    "    correct_test = 0\n",
    "    con_mat_test=np.zeros((5,5))\n",
    "    lossfn1=loss_fn()\n",
    "    file_no=0\n",
    "    count_file_len=0\n",
    "    for test_idx, test_batch, test_labels in data_test:\n",
    "        test_batch, test_labels=test_batch.to(device), test_labels.to(device)\n",
    "        cnn_features = cnn_model(test_batch)\n",
    "        decoder_input = F.one_hot(test_labels % 5, num_classes=5).type(torch.FloatTensor).to(device)\n",
    "        if test_idx[0]==count_file_len:\n",
    "            count_file_len+=file_length_dic_test[str(file_no)]\n",
    "            file_no+=1\n",
    "            print(\"New Patient\")\n",
    "            decoder_input_first = F.one_hot(torch.tensor(0) % 5, num_classes=5).type(torch.FloatTensor).to(device)\n",
    "        else:\n",
    "            decoder_input_first = decoder_input_last\n",
    "        output, decoder_input_last = lstm_model(decoder_input_first, decoder_input, cnn_features)\n",
    "        test_labels_crop = test_labels.view(-1)\n",
    "        test_pred = output.argmax(dim=1,keepdim=True)\n",
    "        if max(test_idx)!=test_idx[-1]:\n",
    "            idx_nump=test_idx.numpy()\n",
    "            max_idx=np.where(idx_nump==max(idx_nump))[0][0]\n",
    "            print(max_idx)\n",
    "            test_pred=test_pred[:max_idx+1]\n",
    "            test_labels_crop=test_labels_crop[:max_idx+1]\n",
    "            output=output[:max_idx+1]\n",
    "        loss1 = lossfn1(output,test_labels_crop).item()\n",
    "        test_loss += test_labels_crop.size()[0]*loss1\n",
    "        correct_test,total_30sec_epochs_test,con_mat_test=conf_mat_create(test_pred,test_labels_crop,correct_test,total_30sec_epochs_test,con_mat_test)\n",
    "    print(\"conf_mat_test:\",con_mat_test)\n",
    "    print(\"total_30sec_epochs_test:\",total_30sec_epochs_test)\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%), Epoch:{}\\n'.format(test_loss/total_30sec_epochs_test, correct_test, total_30sec_epochs_test,100. * correct_test / total_30sec_epochs_test,epoch+1))\n",
    "    sheet1.append([0,1,2,3,4])\n",
    "    for row in con_mat_test.tolist():\n",
    "        sheet1.append(row)\n",
    "\n",
    "if __name__ == '__main__': \n",
    "    batch_size = 24\n",
    "    classes=[0,1,2,3,4]\n",
    "    epochs = 200\n",
    "    seq_length = 8\n",
    "    for k in range(1,2):\n",
    "        path_to_cnn_model = '/RQ1/SleepEDF/cnn_weightedloss_outerloop10_innerloop'+str(k)+'.tar'\n",
    "        path_to_lstm_model = '/RQ1/SleepEDF/lstm_weightedloss_outerloop10_innerloop'+str(k)+'.tar'\n",
    "\n",
    "        path_to_file_length_test='/RQ1/SleepEDF/outerloop10_test.pkl'\n",
    "        f_file_length_test=open(path_to_file_length_test,'rb')\n",
    "        file_length_dic_test=pickle.load(f_file_length_test)\n",
    "        f_file_length_test.close()\n",
    "        \n",
    "        path_to_hdf5_file_test = '/RQ1/SleepEDF/outerloop10_test.hdf5'\n",
    "        \n",
    "        if os.path.isfile(path_to_results):\n",
    "            wb = op.load_workbook(path_to_results)\n",
    "            sheet1 = wb.get_sheet_by_name('Sheet 1')\n",
    "        else:\n",
    "            wb=Workbook()\n",
    "            sheet1=wb.active\n",
    "            sheet1.title = \"Sheet 1\"\n",
    "        \n",
    "        device = 'cpu'\n",
    "        \n",
    "        cnn_model = cnn_models.SleepMultiChannelNet(lstm_option=True) \n",
    "        lstm_model = lstm_models.SleepMultiChannelNet(input_size=2880,hidden_size=256,output_size=5,seq_length=seq_length,device=device)\n",
    "\n",
    "        cnn_model = load_pretrained_model_for_LSTM(cnn_model,path_to_cnn_model)\n",
    "        lstm_model.load_state_dict(torch.load(path_to_lstm_model,map_location=torch.device('cpu'))['state_dict'])\n",
    "\n",
    "        data_gen_test=utils.my_generator1(path_to_hdf5_file_test)\n",
    "        sampler_test=SequentialSampler(data_gen_test)\n",
    "        batch_sampler_test=utils.CustomSequentialLSTMBatchSampler_ReturnAllChunks(sampler_test,batch_size*seq_length,file_length_dic_test,seq_length)\n",
    "        data_test=DataLoader(data_gen_test,batch_size=1,batch_sampler=batch_sampler_test)\n",
    "        print(\"test dataset loaded\") \n",
    "         \n",
    "        test(cnn_model, lstm_model,data_test,epochs)\n",
    "        wb.save(path_to_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
