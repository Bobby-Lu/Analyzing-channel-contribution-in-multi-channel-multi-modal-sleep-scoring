{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Length instances in generator: 2809\n",
      "conf_mat_test: [[ 497.   45.   11.    7.    4.]\n",
      " [  63.   89.   10.    0.   28.]\n",
      " [  10.   34. 1061.   70.   92.]\n",
      " [   0.    0.    5.  165.    0.]\n",
      " [  33.   79.    6.    2.  498.]]\n",
      "total_30sec_epochs_test: 2809\n",
      "\n",
      "Test set: Average loss: 0.5017, Accuracy: 2310/2809 (82%), Epoch:1\n",
      "\n",
      "Total Length instances in generator: 2809\n",
      "conf_mat_test: [[4.530e+02 6.800e+01 2.100e+01 1.500e+01 7.000e+00]\n",
      " [3.900e+01 8.500e+01 2.800e+01 1.000e+00 3.700e+01]\n",
      " [6.000e+00 1.600e+01 1.079e+03 9.600e+01 7.000e+01]\n",
      " [0.000e+00 0.000e+00 1.000e+00 1.690e+02 0.000e+00]\n",
      " [2.900e+01 5.600e+01 2.700e+01 5.000e+00 5.010e+02]]\n",
      "total_30sec_epochs_test: 2809\n",
      "\n",
      "Test set: Average loss: 0.5066, Accuracy: 2287/2809 (81%), Epoch:1\n",
      "\n",
      "Total Length instances in generator: 2809\n",
      "conf_mat_test: [[ 477.   45.   15.   23.    4.]\n",
      " [  48.   67.   20.    0.   55.]\n",
      " [   6.   17. 1054.   73.  117.]\n",
      " [   0.    0.    4.  166.    0.]\n",
      " [  22.   58.   10.    3.  525.]]\n",
      "total_30sec_epochs_test: 2809\n",
      "\n",
      "Test set: Average loss: 0.5010, Accuracy: 2289/2809 (81%), Epoch:1\n",
      "\n",
      "Total Length instances in generator: 2809\n",
      "conf_mat_test: [[ 493.   38.   16.   12.    5.]\n",
      " [  45.   79.   23.    0.   43.]\n",
      " [   7.   13. 1090.   76.   81.]\n",
      " [   0.    0.    2.  168.    0.]\n",
      " [  27.   56.   21.    3.  511.]]\n",
      "total_30sec_epochs_test: 2809\n",
      "\n",
      "Test set: Average loss: 0.4688, Accuracy: 2341/2809 (83%), Epoch:1\n",
      "\n",
      "Total Length instances in generator: 2809\n",
      "conf_mat_test: [[ 496.   49.   11.    8.    0.]\n",
      " [  53.   87.   13.    0.   37.]\n",
      " [  11.   47. 1073.   49.   87.]\n",
      " [   0.    0.    6.  164.    0.]\n",
      " [  27.   74.    6.    0.  511.]]\n",
      "total_30sec_epochs_test: 2809\n",
      "\n",
      "Test set: Average loss: 0.5026, Accuracy: 2331/2809 (83%), Epoch:1\n",
      "\n",
      "Total Length instances in generator: 2809\n",
      "conf_mat_test: [[4.540e+02 6.700e+01 1.700e+01 2.200e+01 4.000e+00]\n",
      " [3.600e+01 9.400e+01 2.200e+01 1.000e+00 3.700e+01]\n",
      " [4.000e+00 2.400e+01 1.064e+03 8.400e+01 9.100e+01]\n",
      " [0.000e+00 0.000e+00 2.000e+00 1.680e+02 0.000e+00]\n",
      " [2.000e+01 6.400e+01 1.700e+01 2.000e+00 5.150e+02]]\n",
      "total_30sec_epochs_test: 2809\n",
      "\n",
      "Test set: Average loss: 0.5147, Accuracy: 2295/2809 (82%), Epoch:1\n",
      "\n",
      "Total Length instances in generator: 2809\n",
      "conf_mat_test: [[ 499.   33.   16.   13.    3.]\n",
      " [  60.   66.   27.    0.   37.]\n",
      " [   9.   21. 1067.   79.   91.]\n",
      " [   0.    0.    4.  166.    0.]\n",
      " [  49.   48.   13.    4.  504.]]\n",
      "total_30sec_epochs_test: 2809\n",
      "\n",
      "Test set: Average loss: 0.5162, Accuracy: 2302/2809 (82%), Epoch:1\n",
      "\n",
      "Total Length instances in generator: 2809\n",
      "conf_mat_test: [[4.660e+02 6.000e+01 2.500e+01 9.000e+00 4.000e+00]\n",
      " [2.800e+01 7.000e+01 3.600e+01 0.000e+00 5.600e+01]\n",
      " [5.000e+00 8.000e+00 1.108e+03 5.100e+01 9.500e+01]\n",
      " [0.000e+00 0.000e+00 8.000e+00 1.620e+02 0.000e+00]\n",
      " [1.400e+01 5.500e+01 1.800e+01 1.000e+00 5.300e+02]]\n",
      "total_30sec_epochs_test: 2809\n",
      "\n",
      "Test set: Average loss: 0.4886, Accuracy: 2336/2809 (83%), Epoch:1\n",
      "\n",
      "Total Length instances in generator: 2809\n",
      "conf_mat_test: [[5.280e+02 1.500e+01 1.600e+01 5.000e+00 0.000e+00]\n",
      " [6.900e+01 5.100e+01 4.300e+01 0.000e+00 2.700e+01]\n",
      " [1.000e+01 7.000e+00 1.146e+03 5.100e+01 5.300e+01]\n",
      " [0.000e+00 0.000e+00 4.000e+00 1.660e+02 0.000e+00]\n",
      " [5.000e+01 3.600e+01 5.000e+01 1.000e+00 4.810e+02]]\n",
      "total_30sec_epochs_test: 2809\n",
      "\n",
      "Test set: Average loss: 0.4654, Accuracy: 2372/2809 (84%), Epoch:1\n",
      "\n",
      "Total Length instances in generator: 2809\n",
      "conf_mat_test: [[ 421.   83.   45.    6.    9.]\n",
      " [  16.   53.   35.    0.   86.]\n",
      " [   4.    6. 1119.   29.  109.]\n",
      " [   0.    0.   23.  147.    0.]\n",
      " [   5.   30.   18.    0.  565.]]\n",
      "total_30sec_epochs_test: 2809\n",
      "\n",
      "Test set: Average loss: 0.4967, Accuracy: 2305/2809 (82%), Epoch:1\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import h5py    \n",
    "import numpy as np  \n",
    "from sklearn.metrics import confusion_matrix\n",
    "import torch.optim as optim\n",
    "from sklearn.metrics import accuracy_score\n",
    "import utils\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "import os\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "from collections import OrderedDict\n",
    "from openpyxl import Workbook\n",
    "import openpyxl as op\n",
    "import cnn_models\n",
    "\n",
    "def loss_fn():\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    return criterion\n",
    "\n",
    "def conf_mat_create(predicted,true,correct,total_30sec_epochs,conf_mat):\n",
    "    total_30sec_epochs+=true.size()[0]\n",
    "    correct += predicted.eq(true.view_as(predicted)).sum().item()\n",
    "    conf_mat=conf_mat+confusion_matrix(true.cpu().numpy(),predicted.cpu().numpy(),labels=classes)\n",
    "    return correct, total_30sec_epochs,conf_mat\n",
    "\n",
    "def load_model(model,path):\n",
    "    state_dict_new_model=model.state_dict()\n",
    "    checkpoint = torch.load(path,map_location=torch.device('cpu'))\n",
    "    state_dict=checkpoint['state_dict']\n",
    "    state_dict_remove_module = OrderedDict()\n",
    "    for k, v in state_dict.items():\n",
    "        #k = k[7:] # remove `module.`\n",
    "        state_dict_remove_module[k] = v\n",
    "    #print(state_dict_remove_module.keys())\n",
    "    state_dict_new_model.update(state_dict_remove_module)\n",
    "    model.load_state_dict(state_dict_new_model)\n",
    "    return model\n",
    "\n",
    "def test(cnn_model,data_test,epoch):\n",
    "    cnn_model.eval()\n",
    "    total_30sec_epochs_test = 0\n",
    "    test_loss = 0.0\n",
    "    correct_test = 0\n",
    "    con_mat_test=np.zeros((5,5))\n",
    "    lossfn=loss_fn()\n",
    "    for test_idx, test_data, test_labels in data_test:\n",
    "        test_data, test_labels=test_data.to(device), test_labels.to(device)\n",
    "        output = cnn_model(test_data)\n",
    "        test_labels_crop = test_labels.view(-1)\n",
    "        test_pred = output.argmax(dim=1,keepdim=True)\n",
    "        loss = lossfn(output,test_labels_crop).item()\n",
    "        test_loss += test_labels_crop.size()[0]*loss\n",
    "        correct_test,total_30sec_epochs_test,con_mat_test=conf_mat_create(test_pred,test_labels_crop,correct_test,total_30sec_epochs_test,con_mat_test)\n",
    "    print(\"conf_mat_test:\",con_mat_test)\n",
    "    print(\"total_30sec_epochs_test:\",total_30sec_epochs_test)\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%), Epoch:{}\\n'.format(test_loss/total_30sec_epochs_test, correct_test, total_30sec_epochs_test,100. * correct_test / total_30sec_epochs_test,epoch+1))\n",
    "    sheet1.append([0,1,2,3,4])\n",
    "    for row in con_mat_test.tolist():\n",
    "        sheet1.append(row)\n",
    "    \n",
    "        \n",
    "if __name__ == '__main__': \n",
    "    batch_size = 192\n",
    "    classes=[0,1,2,3,4]\n",
    "    epochs = 200\n",
    "    use_cuda = torch.cuda.is_available()\n",
    "    device = torch.device(\"cuda:0\" if use_cuda else \"cpu\")\n",
    "    \n",
    "    for k in range(1,2):\n",
    "        path_to_cnn_model = '/RQ1/SleepEDF/WeightedLoss/models/cnn/cnn_weightedloss_outerloop10_innerloop'+str(k)+'.tar'\n",
    "        path_to_hdf5_file_test = '/RQ/SleepEDF/outerloop10_test.hdf5'\n",
    "        \n",
    "        if os.path.isfile(path_to_results):\n",
    "            wb = op.load_workbook(path_to_results)\n",
    "            sheet1 = wb['Sheet 1']\n",
    "        else:\n",
    "            wb=Workbook()\n",
    "            sheet1=wb.active\n",
    "            sheet1.title = \"Sheet 1\"\n",
    "\n",
    "        dataset_test = utils.my_generator1(path_to_hdf5_file_test)\n",
    "        data_test = DataLoader(dataset_test, batch_size=192, shuffle=False, sampler=None,batch_sampler=None)\n",
    "\n",
    "        cnn_model = cnn_models.SleepMultiChannelNet(lstm_option=False) \n",
    "        model = load_model(cnn_model, path_to_cnn_model)\n",
    "        test(cnn_model,data_test,epoch=0)\n",
    "        wb.save(path_to_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
